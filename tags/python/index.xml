<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on Terminal</title>
    <link>http://avi.im/blag/tags/python/</link>
    <description>Recent content in python on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Feb 2016 17:03:00 +0530</lastBuildDate><atom:link href="http://avi.im/blag/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Catching SIGTERM in Python</title>
      <link>http://avi.im/blag/2016/sigterm-in-python/</link>
      <pubDate>Sat, 20 Feb 2016 17:03:00 +0530</pubDate>
      
      <guid>http://avi.im/blag/2016/sigterm-in-python/</guid>
      <description>I needed a very simple SIGTERM handler in a script I was working on. It is very simple to do so in Python, add a handler method and &amp;lsquo;register&amp;rsquo; it.
First define a handler method:
def sigterm_handler(signal, frame): # this method defines the handler i.e. what to do # when you receive a SIGTERM pass  And register the handler:
# Register the handler and let the process know that # there is a handler for SIGTERM signal.</description>
    </item>
    
    <item>
      <title>Using uWSGI with Python 3</title>
      <link>http://avi.im/blag/2015/uwsgi-python3/</link>
      <pubDate>Fri, 18 Dec 2015 14:03:00 +0530</pubDate>
      
      <guid>http://avi.im/blag/2015/uwsgi-python3/</guid>
      <description>Gevent does not have Python 3 support yet, but its available as a release candidate. So this will lead to problems if you use the latest stable Gevent with Python 3. Follow these simple steps to install uSWGI and Gevent properly for Python 3.
  Uninstall existing uWSGI:
 pip uninstall uwsgi pip3 uninstall uwsgi    Install everything via pip3 and use Gevent v1.1rc3:
 pip install gevent==1.</description>
    </item>
    
    <item>
      <title>When is my Cake Day?</title>
      <link>http://avi.im/blag/2015/kekday/</link>
      <pubDate>Fri, 20 Nov 2015 22:20:00 +0530</pubDate>
      
      <guid>http://avi.im/blag/2015/kekday/</guid>
      <description>Reddit gives all the user info in a handy JSON at this URL: https://www.reddit.com/user/&amp;lt;username here&amp;gt;/about.json
example: https://www.reddit.com/user/spez/about.json
The created_utc field in data is the date of user&amp;rsquo;s registration aka Cake Day in unix epoch format (in UTC) and we can easily convert that to readable format:
&amp;gt;&amp;gt;&amp;gt; import time &amp;gt;&amp;gt;&amp;gt; time.strftime(&amp;#34;%D&amp;#34;, time.gmtime(1118030400)) &amp;#39;06/06/05&amp;#39; Using Python Requests, we can turn this into a handy function:
import time import requests def get_my_cake_day(username): url = &amp;#34;https://www.</description>
    </item>
    
    <item>
      <title>I discovered a vulnerability on Gaana.com before the main POC hack</title>
      <link>http://avi.im/blag/2015/hacking-gaana/</link>
      <pubDate>Sun, 31 May 2015 19:20:00 +0530</pubDate>
      
      <guid>http://avi.im/blag/2015/hacking-gaana/</guid>
      <description>Update: I have removed the blog post. TLDR; On 12th of May I had discovered a vulnerability on Gaana.com. I contacted their team and it was fixed recently. I had written blog post in detail about it, now I think it&amp;rsquo;s best I remove everything. Thanks for reading!
 </description>
    </item>
    
    <item>
      <title>Scraping Javascript page using Python</title>
      <link>http://avi.im/blag/2014/scraping-javascript-website-python/</link>
      <pubDate>Sat, 18 Oct 2014 14:03:00 +0530</pubDate>
      
      <guid>http://avi.im/blag/2014/scraping-javascript-website-python/</guid>
      <description>Python library dryscape can be used to scrape javascript driven websites.
Code To give an example, I created a sample page with following HTML code. (link):
&amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt; &amp;lt;title&amp;gt;Javascript scraping test&amp;lt;/title&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;p id=&#39;intro-text&#39;&amp;gt;No javascript support&amp;lt;/p&amp;gt; &amp;lt;script&amp;gt; document.getElementById(&#39;intro-text&#39;).innerHTML = &#39;Yay! Supports javascript&#39;; &amp;lt;/script&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt;  without javascript it says: No javascript support and with javascript: Yay! Supports javascript
Scraping without JS support: &amp;gt;&amp;gt;&amp;gt; import requests &amp;gt;&amp;gt;&amp;gt; from bs4 import BeautifulSoup &amp;gt;&amp;gt;&amp;gt; response = requests.</description>
    </item>
    
  </channel>
</rss>
